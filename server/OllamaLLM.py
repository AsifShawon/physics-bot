from langchain_community.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import SystemMessage, AIMessage, HumanMessage
from pdfToVectoreStore import search
from collections import deque
from dataclasses import dataclass
from typing import List, Optional
import time

@dataclass
class ConversationTurn:
    question: str
    answer: str
    timestamp: float
    context: Optional[str] = None

class ChatHistoryManager:
    def __init__(self, max_turns=3, max_age_minutes=30):  # Reduced max_turns
        self.max_turns = max_turns
        self.max_age_minutes = max_age_minutes
        self.history = deque(maxlen=max_turns)
        self.system_message = """You are a helpful physics assistant, committed to delivering clear and accurate explanations. Always accompany your explanations with relevant examples, either from the provided physics book or generated by yourself.

For questions outside the domain of physics, respond with: I can assist only with physics-related queries.

You will be given additional context to guide your responses. Focus strictly on physics and avoid answering unrelated questions."""

    def add_turn(self, question: str, answer: str, context: Optional[str] = None):
        self.history.append(ConversationTurn(
            question=question,
            answer=answer,
            timestamp=time.time(),
            context=context
        ))

    def get_relevant_history(self) -> str:
        current_time = time.time()
        formatted_history = []
        
        for turn in self.history:
            if (current_time - turn.timestamp) / 60 <= self.max_age_minutes:
                formatted_history.append(f"Human: {turn.question}")
                formatted_history.append(f"Assistant: {turn.answer}")
        
        return "\n".join(formatted_history) if formatted_history else "No previous conversation context."

    def clear_history(self):
        self.history.clear()

# Global instances
history_manager = ChatHistoryManager(max_turns=3)  # Reduced max_turns
ollama_model = None

def initialize_model(model_name="gemma2:2b-instruct-q4_K_M"):
    global ollama_model
    if ollama_model is None:
        ollama_model = Ollama(model=model_name)
    return ollama_model

def clear_chat_history():
    history_manager.clear_history()
    initialize_model()

def get_prompt_template(query: str) -> str:
    if any(keyword in query.lower() for keyword in ['calculate', 'solve', 'find the value']):
        return """Previous Relevant Discussion:
{chat_history}

Current Question: {query}

Context: {context}

This question requires a calculation. Please:
1. Use relevant information from previous discussion if applicable
2. Show the step-by-step calculation process
3. Provide the final answer with units"""
    
    elif any(keyword in query.lower() for keyword in ['what is', 'define', 'explain']):
        return """Previous Relevant Discussion:
{chat_history}

Current Question: {query}

Context: {context}

Please provide a conceptual explanation, connecting to our previous discussion if relevant."""
    
    else:
        return """Previous Relevant Discussion:
{chat_history}

Current Question: {query}

Context: {context}

Please provide a clear answer, referencing our previous discussion where relevant."""

def is_follow_up(query: str) -> bool:
    follow_up_keywords = [
        'tell me more', 'describe', 'give an example', 'explain further', 
        'clarify', 'elaborate', 'more details', 'expand on', 'what about'
    ]
    return any(keyword in query.lower() for keyword in follow_up_keywords)

def generate_text(query: str) -> str:
    # Initialize model if not already done
    model = initialize_model()
    
    # Get context based on query type
    if is_follow_up(query):
        context = "Using previous conversation context."
    else:
        retrieved_text = search(query)
        context = (retrieved_text if retrieved_text != "No relevant docs were retrieved using the relevance score threshold 0.5"
                  else "Using general physics knowledge and conversation context.")

    # Get prompt template and format it
    prompt_template = get_prompt_template(query)
    chat_history = history_manager.get_relevant_history()
    
    full_prompt = prompt_template.format(
        query=query,
        context=context,
        chat_history=chat_history
    )

    try:
        # Generate response
        response = model.invoke(full_prompt)
        response = StrOutputParser().parse(response)
        
        # Add to history
        history_manager.add_turn(query, response, context)
        
        # Return response character by character for streaming
        return response
    except Exception as e:
        error_message = f"Error generating response: {e}"
        return error_message