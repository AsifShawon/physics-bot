from langchain_community.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import SystemMessage, AIMessage, HumanMessage
from pdfToVectoreStore import search
from collections import deque
from dataclasses import dataclass
from typing import List, Optional
import time

@dataclass
class ConversationTurn:
    question: str
    answer: str
    timestamp: float
    context: Optional[str] = None

class ChatHistoryManager:
    def __init__(self, max_turns=5, max_age_minutes=30):
        self.max_turns = max_turns
        self.max_age_minutes = max_age_minutes
        self.history = deque(maxlen=max_turns)
        self.system_message = """You are a helpful physics assistant, committed to delivering clear and accurate explanations. Always accompany your explanations with relevant examples, either from the provided physics book or generated by yourself.

For questions outside the domain of physics, respond with: I can assist only with physics-related queries.

You will be given additional context to guide your responses. Focus strictly on physics and avoid answering unrelated questions."""

    def add_turn(self, question: str, answer: str, context: Optional[str] = None):
        self.history.append(ConversationTurn(
            question=question,
            answer=answer,
            timestamp=time.time(),
            context=context
        ))

    def get_relevant_history(self) -> str:
        current_time = time.time()
        formatted_history = []
        
        for turn in self.history:
            if (current_time - turn.timestamp) / 60 <= self.max_age_minutes:
                formatted_history.append(f"Human: {turn.question}")
                formatted_history.append(f"Assistant: {turn.answer}")
        
        return "\n".join(formatted_history) if formatted_history else "No previous conversation context."

    def clear_history(self):
        self.history.clear()

# Global instances
history_manager = ChatHistoryManager(max_turns=5)
ollama_model = None

def initialize_model(model_name="gemma2:2b-instruct-q4_K_M"):
    global ollama_model
    if ollama_model is None:
        ollama_model = Ollama(model=model_name)
    return ollama_model

def clear_chat_history():
    history_manager.clear_history()
    initialize_model()

def get_prompt_template(query: str, is_math_follow_up=False) -> str:
    """
    Generate a prompt template based on the query type.
    If it's a math follow-up, use a calculation-focused prompt template.
    """
    if is_math_follow_up:
        return """Previous Relevant Discussion:
{chat_history}

Current Question: {query}

Context: {context}

This is a mathematical follow-up. Please:
1. Refer to any related prior conversation or calculations.
2. Provide detailed step-by-step calculations.
3. Present the final answer with units and clarify assumptions or approximations used."""
    
    elif any(keyword in query.lower() for keyword in ['calculate', 'solve', 'find the value']):
        return """Previous Relevant Discussion:
{chat_history}

Current Question: {query}

Context: {context}

This question requires a calculation. Please:
1. Use relevant information from previous discussion if applicable
2. Show the step-by-step calculation process
3. Provide the final answer with units"""
    
    elif any(keyword in query.lower() for keyword in ['what is', 'define', 'explain']):
        return """Previous Relevant Discussion:
{chat_history}

Current Question: {query}

Context: {context}

Please provide a conceptual explanation, connecting to our previous discussion if relevant."""
    
    else:
        return """Previous Relevant Discussion:
{chat_history}

Current Question: {query}

Context: {context}

Please provide a clear answer, referencing our previous discussion where relevant."""

def is_follow_up(query: str) -> bool:
    """
    Determine if the query is a follow-up question, specifically for math if needed.
    """
    follow_up_keywords = [
        'tell me more', 'describe', 'give an example', 'explain further', 
        'clarify', 'elaborate', 'more details', 'expand on', 'what about', 'how about',
        'further explanation', 'more information', 'more context', 'more details',
        'more examples', 'more explanation', 'more clarification', 'more elaboration',
    ]
    math_follow_up_keywords = [
        'calculate', 'step-by-step', 'how did you', 'derive', 'solution', 'math details', 'mathematically', 
        'show the math', 'mathematical explanation', 'mathematical example', 'math steps', 'math process', 
        'math calculation', 'math formula', 'math concept', 'math reasoning', 'math derivation', 'math proof', 
        'mathematical reasoning', 'mathematical derivation', 'mathematical proof', 
    ]
    
    if any(keyword in query.lower() for keyword in math_follow_up_keywords):
        return "math"
    elif any(keyword in query.lower() for keyword in follow_up_keywords):
        return "general"
    else:
        return False

def generate_text(query: str) -> str:
    # Initialize model if not already done
    model = initialize_model()
    
    # Get context based on query type
    follow_up_type = is_follow_up(query)
    is_math_follow_up = follow_up_type == "math"
    
    if follow_up_type:
        context = "Using previous conversation context."
    else:
        retrieved_text = search(query)
        context = (retrieved_text if retrieved_text != "No relevant docs were retrieved using the relevance score threshold 0.5"
                  else "Using general physics knowledge and conversation context.")

    # Get prompt template and format it
    prompt_template = get_prompt_template(query, is_math_follow_up=is_math_follow_up)
    chat_history = history_manager.get_relevant_history()
    
    full_prompt = prompt_template.format(
        query=query,
        context=context,
        chat_history=chat_history
    )

    try:
        # Generate response
        response = model.invoke(full_prompt)
        response = StrOutputParser().parse(response)
        
        # Add to history
        history_manager.add_turn(query, response, context)
        
        # Return response character by character for streaming
        return response
    except Exception as e:
        error_message = f"Error generating response: {e}"
        return error_message
